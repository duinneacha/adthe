\chapter{Background}
\label{chap:background}
\lhead{\emph{Background}}
The key question to answer in this chapter is: "What has been done/is being done". 

This chapter comprises around 4000 words and should put your project into context within Computer Science. Your focus here should be on the final section "Current State of the Art". This should be at least 2500 of the 4000 words of this section.

System Architecture

System architects are the designated experts responsible for a system’s architecture as well as the technical standards of a product. This includes technologies, platforms and infrastructure. They set the vision and their analysis is key to the product’s successful definition, design, delivery and life-time support. They therefore need to understand not only what the business need, but also what is logical, scalable, cost effective and in-line with the overall technology goals of the organisation. \cite{Clements2013}

One of the vital skills of an architect is to be able to view the architecture from many different standpoints: each one of them individually might not be fully relevant but combining them together gives a helicopter view of the product. These standpoints comprise of principles, standards, patterns and anti-patterns, rules of thumb and empirical practices which are essential for decision making towards a particular direction and also evaluating the project’s success. \cite{Koen2019}


SOLID Principles
The SOLID principles do not only apply on software development but also when architecting a system.

Single Responsibility Principle

Each system capability (e.g. service/module/api) should have only one responsibility and as such one reason to change. Keeping the responsibilities as narrow as possible means that the users know of the intended purpose, which leads to less errors.

Open-Closed Principle

This principle proposes that it is preferable to extend a system behaviour, without modifying it. Although it is often not a good idea to try to anticipate changes in requirements ahead of time (as it can lead to over-complex designs), being able to adapt new functionality with minimum changes to existing components is key to the application’s longevity. \cite{Baniassad2018}

Liskov Substitution Principle

In Software Development, this means that derived classes must be substitutable for their base classes, but this principle’s resemblance with Bertrand Meyer’s Design by Contract is how it can be applied to Distributed Architecture: two services communicate effectively and repeatedly when there is a common ‘contract’ between them, which defines the inputs/outputs, their structure and their constraints. Therefore: given two distributed components with the same contract, one should be replaceable with other component with the same contract without altering the correctness of the system. \cite{Baniassad2018}

Interface Segregation Principle

Interfaces/contracts must be as fine grained as possible and client specific, so calling clients do not depend on functionality they don’t use. This goes hand in hand with the Single Responsibility principle: by breaking down interfaces, we favour Composition by separating by roles/responsibilities, and Decoupling by not coupling derivative modules with unneeded responsibilities. \cite{Noback2018}

Dependency Inversion Principle
High level modules should not depend on low level ones; they should both depend on abstractions. Likewise, abstractions should not depend on details, but details should depend on abstractions. As such this principle introduces an interface abstraction between higher-level and lower-level software components or layers to remove the dependencies between them.


The ’Least’ Principles

The principle of Least Astonishment

The principle of least astonishment (or Least Surprise) \cite{Koen2019} suggests that a solution or approach would not surprise a reasonably knowledgeable person in the subject area when encountered for the first time (the audience may vary e.g. end-user, programmer, tester etc). In more practical terms, the principle aims to leverage the pre-existing knowledge of users to minimise their learning curve when using a module, so anything with high unpredictability factor is a good candidate for re-design.
It applies to every single aspect of the architecture: from naming services, to the visualisation of user interfaces, to the design of the domain model.

The principle of Least Effort

This principle (also called Zipf’s Law) \cite{Kanwal2017} stems from a basic human behaviour: Everyone tends to follow the path that is as close to effortless as possible. So for example if our design follows a particular pattern, the next developer will follow the same pattern again and again unless there is a significantly easier way to perform the task, in which case they will change! Or, taking this further, once they find acceptable results for a task, there is no immediate need to improve the current solution. 

Least effort is a variant of least work

As such it is imperative to aim for a strong start by putting the right architecture in place: it sets high expectations and ensures that the quality is not compromised in the project’s lifecycle and it will be adhered to in case of future changes. Once the right design is in place, and architectural framework can be established which will be the bases for future projects of a similar architectural style.


The principles of Economics

The cost of making the most of an opportunity and the cost of delaying making decisions.

The principle of Opportunity Cost
Every time we make a choice, there is a certain value we place on that choice. Value has two parts: benefits and costs. The opportunity cost of a choice is what we give up to get it. To make a good economic decision, we want to choose the option with the greatest benefit to us but the lowest cost. \cite{Raymond2003}

Architecture is weighing choices against each other and trying to make an informed decision on which one will add the most value for the project. For instance, a very common dichotomy is whether to create a tactical solution with quick time to market or a more strategic one which will be more expensive now with the view to leverage it in future projects and hence minimise the cost later down the line.

The principle of Last Responsible Moment
This principle (aka Cost of Delay) originates from Lean Software Development \cite{Dombrowski2014} and emphasises holding on taking important actions and crucial decisions for as long as possible. This is done so as to not eliminate important alternatives until the last possible moment i.e. wait to narrow the options down until you are better informed.
A strategy of not making a premature decision but instead delaying commitment and keeping important and irreversible decisions open until the cost of not making a decision becomes greater than the cost of making a decision.

One way to mitigate the risk of deciding too late is to build Proof of Concepts (POCs) to prototype the alternative options and demonstrate to the stakeholders what they are asking for.



Data Visualisation

Data visualization usually represents graphs and charts that are visual representations of data. These graphical displays provide a powerful way of summarizing and presenting data in a way that most people find easier to comprehend. Charts and graphs enable us to see the main features or characteristics of the data. The graphs not only enable us to present the numerical findings of a study, but also provide the shape and pattern of the data which are critical in data analysis and decision making. It is said that a picture is worth a thousand words; this is particularly true when a large set of data is effectively presented using charts and graphs that quickly reveal important features. Visual displays of data are easily recognizable and are found ubiquitously in business periodicals, financial magazines, on the Internet, and televisions. \cite{VisualizationOverview}

Suraj Thatte writes in his article that data visualization has a strong design element to it. Given the differences in domains, applications and audience it’s hard to put a structure around the best way to visualize your data. \cite{Thatte2019}

Suraj shares his tips on representing data visually as follows:

	Choose the right visual - Always remember that “form follows function” – purpose of a visual should be the starting point of its design
	
	The question that should be asked is what are you visualising - The visualisation for this project is Project Budget breakdown displaying the Total Budget, Total Remaining and totals for Income, Staff and Other. A good fit for this project is a Pie Chart which will clearly highlight the Total Remaining against the all other fields.

	Trivial are many but vital are few (data points) - Only show the data that the audience is interested in.

	Visuals should reflect reality and not distort them. Formatting of the chart plays an important role as it sets up a frame of reference for the audience.

	Use of colour should be made to add more information or to highlight key data points in a visual. In all other cases it is redundant and distracting. Lisa Charlotte Rost in her article on considering colours states that using as few contrasting colours is good, avoid gradients and consider visually impaired members of the audience. Colours should be made to add more information to a chart, in all other cases it is redundant and distracting. \cite{Rost2018}

A visual chart should also avoid overdoing the aesthetic elements which the audience may find distracting. One of the seven wastes in the Lean Philosophy is ‘Over-processing’. \cite{Dombrowski2014}


Web Scraping

As the name implies, it's a method of 'scraping', harvesting or extracting data from webpages. It is one of the oldest techniques for extracting web content. \cite{Glez-Pena2013a} Web scraping software may access the world wide web using HTTP or through a web browser. While web scraping can be done manually by a software user, the term typically refers to automated processes using a bot or a web crawler. It is a form of copying in which specific data is gathered and copied from the web, typically into a local database or spreadsheet for later retrieval or analysis.

Web scraping a web page involves fetching it and extracting from it. Fetching is the downloading of a page which a browser does when a user views the page, therefore web crawling is a main component of web scraping to fetch pages for later processing. Once fetched, then extraction can take place.
The content of a page may be parsed, searched, reformatted or have its data copied into a database or spreadsheet. Web scrapers typically take something out of a page to make use of it for another purpose somewhere else. Within this project the author will design a program to search the relevant news sites looking for articles of companies within the Nimbus contacts database so that they may be kept up to date of news articles relating to their contacts. 

Web scraping is used for content scraping and as a component for applications used for web indexing, data mining, online price change monitoring and comparison, product review, to watch the competition, gather real estate listings, weather data monitoring, web site change detection, research and many other uses.

Web pages are built using text based markup languages HTML and XHTML and frequently contain a wealth of useful data in text form. However most web pages are designed for human end users and not for ease of automated use, because of this, toolkits that scrape content were created. A web scraper is an API to extract data from a web site. Newer forms of web scraping involve listening to data feeds from web servers e.g. it is commonly used as a transport storage mechanism between the client and a web server.
There are methods that some web sites use to prevent web scraping such as detecting and preventing bots from crawling and viewing their pages. In response there are web scraping systems that rely on using techniques and DOM parsing, computer vision and natural language processing to simulate human browsing to enable gathering web page content for offline parsing.

The author has focused in on the Python library called BeautifulSoup. Beautiful Soup is a Python library for parsing structured data. It allows a developer to interact with HTML in a similar way to how someone would interact with a web page using developer tools within a browser. Beautiful Soup exposes a couple of intuitive functions you can use to explore the HTML you received. It is possible to parse through web page content by any element with a ‘class’ or ‘id’ tag attached to it and extract text content from within.


\section{Thematic Area within Computer Science}
Position your topic within Computer Science. This activity will aid you in your literature review also. We zoom out to see three levels: \cite{Barricelli2019}

% notice the enumerate structure to create itemized lists
\begin{enumerate}
    \item What is the core topic your project is about? e.g., Mobile app for online voting.
    \item What core area(s) does the project fall under? e.g., Mobile applications, Social Networking, Service Providers. 
    \item What main area(s) of Computer Science does the project fall under? e.g. Software Development, Cloud Computing.
\end{enumerate}

The ACM Computing Classification System (http://www.acm.org/about/class) will aid you in this, use the 2012 categories. Make sure to use figures and illustrations were appropriate. LaTeX will take care of the formatting of these. Do not try to get fancy here, you should concentrate on the content and not the formatting, this is why we are specifying LaTeX.

% Again take note of the structure, simply copy and paste this for future single figures
\begin{figure}[ht]
  \centering
      \includegraphics[width=0.7\textwidth]{successkid.jpg}
  \caption[A picture of the success kid!]{A picture of the success kid!\cite{Reference1}}
  \label{fig:successkid}
\end{figure}

You can specify the width and label for a figure which allows you to reference the figure and you can attribute a source in the figure caption as is done for figure \ref{fig:successkid}. Make sure you reference all external figures (i.e. figures you did not create yourself). Also use references for all figures e.g. use "... in figure \ref{fig:successkid} ..." NOT "... in the figure above ...".

\section{A Review of -INSERT THEMATIC AREA-}
The focus of this section is at the heart of the project research phase. You must identify the main sources of information you should be aware of within your chosen area and pay regular attention to so as to strengthen your knowledge in the core topic you are working at. So here you should develop an knowledge of not only your core topic but also about the area of computer science the topic falls under. More specifically you should research the following:
\begin{itemize}
    \item The top 5 International Conferences and Journals most related to your topic. This is crucial, as it represents the main source for keeping you aware of what the state-of-the-art in your topic is.
    \begin{itemize}
        \item In particular it will make you aware of what other projects related to yours have been already done (so that you can compare/position your project w.r.t. these).
        \item What new techniques are being developed, so that you can apply them in your work. e.g. new frameworks for data visualization
    \end{itemize}
    \item The top 3 most recent books/texts related to your topic. There are many free resources from which you may download a relevant text on the topic of your project. Try to either download or borrow 3 recent (no older than 10 years) texts relating to the topic your project is on which you will use throughout the project as reference material and to aid in tackling a number of the technical problems you may encounter. Any PhD/MSc thesis that have published in the last 5 years relating to the topic are also invaluable resources as they will contain a state of the art and references in your project topic. Approach these only after reading/viewing the wikis/Youtube videos you find as a certain level of knowledge will be assumed about the topic.
    \item The top 5 companies/organizations potentially interested in the product you are developing. Finally, this is also crucial, as it forces you extend to purely programmer view of the project to a wider view considering the market, potential stakeholders and niches where your product can become useful. Moreover, Computer Science is a huge topic with loads of different works and roles. If you pick a project in the area you feel passionate about, and you identify what the market in this area is about, then you can drive your future professional career (from the very beginning) towards the path that makes you happier. I know that this does sound as a very technical reason, but I suppose we all agree is probably the most important of all reasons for choosing a particular project focus. 
    \item The top 5 wiki/forums/blogs/Youtube channels most related to your topic. This is crucial to you as well, as it represents a more accessible, personal and less informal way of communication with people working/interested on the same topic as you are. This communication is extremely helpful for improving your skills, solving potential doubts and increase the interest/relevance of the topic/area itself.
\end{itemize}

You should begin your journey of discovery in reverse order to the listing above (which is given in order of academic importance/significance). So when you are researching your topic first look up some TedX talks or youtube tutorials, then research what companies are doing in the area, then get a handful of very good texts on the core topics of your area (anything older than 5 years usually is not helpful here) and finally start reading conference or journal papers (again newer is better here). In particular during this section you may need to use tables to list resources. These are also automatically formatted in latex thus allowing you to concentrate on content. for example table \ref{tab:Mylar}.

\begin{table}[ht]
	\centering
		\begin{tabular}{ c  c  }
		\hline
		\hline
		Parameter & PET \\
		\hline
		Youngs Modulus & 2800-3100MPa \\
		Tensile Strength & 55-75MPa \\
		Glass Temperature & 75$^\circ$C \\
		Density & 1400kg/m$^3$ \\
		Thermal Conductivity & 0.15-0.24Wm$^{-1}$K$^{-1}$ \\
		Linear Expansion Coefficient & $7\times10^-5$ \\
		Relative Dielectric Constant @ 1MHz & 3\\
		Dielectric Breakdown Strength & 17kVmm$^{-1}$\\
		\end{tabular}
	\caption{PET Physical Properties}
	\label{tab:Mylar}
\end{table}

What has been done before in your community w.r.t. your topic? Once you have gotten an understanding of the topic and technologies and have identified the top 5 formal conferences/journals, wiki/forums/blogs/Youtube channels and companies/organizations the next step is to research in depth on them! And here in depth means in depth. Make sure you cite\cite{Reference1} a number of papers \cite{Reference3}, luckily Latex will take care of the ordering of the citations \cite{Reference2} for you.

The aim here is that you find the trends in your topic (3), and more in general in the area in which your topic resides (2) your project falls under and from these trends you develop your initial project question further and begin to get insights into how others have solved/approached similar problems. Think of this section as colouring in your initial idea. Before you approach this section you should read at least 4/5 good literature reviews (a selection of last years projects will be posted on blackboard to aid you but you should find other sources also).

In particular in this section, you must find and analyze at least 5 (ideally around 10) works belonging to, or at least related to, your work. You must describe these works and position your project w.r.t. them (i.e., clearly identify the similarities and differences between your project and each of these works). Also remember if you find that you are detailing topics that you have not introduced already here you need to add something to the earlier Scope section.